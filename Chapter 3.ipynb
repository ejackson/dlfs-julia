{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16905662-eb76-4b67-a9c7-bdd68c297efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get to a deep learning model from some first principles.  Comment heavy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68e7df-8796-4206-84b3-91387dfb8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This type definitions need to be in a module, so that they can be redefined without headache\n",
    "module Layers\n",
    "using IterTools\n",
    "using Zygote\n",
    "using Functors\n",
    "\n",
    "# This (immutable) type closes over the non-linear function, the weights and the bias vector\n",
    "struct Dense{F, M<:AbstractMatrix, B}\n",
    "    σ::F\n",
    "    weights::M\n",
    "    bias::B\n",
    "end\n",
    "@functor Dense\n",
    "\n",
    "# Here is the convenience outer constructor, which takes the dimensions of the Layer and the optional non-linearity\n",
    "# Notice that it uses the default Dense constructor\n",
    "# For now we assume that bias is not-optional\n",
    "# Lets say that x, y is input, output dimensions.  Going to assume matrices, not tensors to keep it easy\n",
    "function Dense(x, y, σ=identity)\n",
    "    Dense(σ, rand(y,x), ones(y, 1))\n",
    "end\n",
    "\n",
    "# And the magic of dispatch turns the Layer into a function, so we can use the rest of the language to compose it\n",
    "function (a::Dense)(x::AbstractVecOrMat)\n",
    "  return a.σ.(a.weights * x .+ a.bias)\n",
    "end\n",
    "\n",
    "# Implement size protocol to get dimension checking\n",
    "function size(l::Dense) \n",
    "    return Base.size(transpose(l.weights))\n",
    "end\n",
    "\n",
    "# ---------- Compose Layers into chain ------\n",
    "# Convenience function to check whether two sizables conform\n",
    "function conform((m1,m2))\n",
    "    size(m1)[2] == size(m2)[1]\n",
    "end\n",
    "\n",
    "struct Chain{L}\n",
    "    layers::L\n",
    "end\n",
    "@functor Chain\n",
    "\n",
    "# Constructor checks the sizes of the layers, throws up as appropriate\n",
    "function Chain(ls...)\n",
    "    all(conform, partition(ls, 2, 1)) ? Chain(ls) : error(\"Layers in chain do not conform, dummy\")\n",
    "end\n",
    "\n",
    "# Treat the chain as a function so we keep FP abstractions\n",
    "# ♡ So we reverse and splat the layers into function composition. ♡\n",
    "function (c::Chain)(x::AbstractVecOrMat)\n",
    "    return ∘(reverse(c.layers)...)(x)\n",
    "end\n",
    "\n",
    "\n",
    "#-------- Training ----------\n",
    "# Here we'll use auto differentiation, because its a primitive now :)\n",
    "# Trick 1: Zygote will automatically seek out and differentiate with respect to keys in map, so because our layers are maps this happens automatically\n",
    "# Trick 2: Functors.\n",
    "function l2(ŷ, y)\n",
    "    sum((ŷ - y).^2)\n",
    "end\n",
    "\n",
    "\n",
    "# Pass in the observed x and target ys, a loss function and the chain and it will return the optimized chain\n",
    "function train(x, y, loss, chain, η=0.01f0)\n",
    "    # Take a basic, 1000 iteration, approach\n",
    "    count=0\n",
    "    while(count < 1000)\n",
    "        ∇ = gradient(ch -> loss(y, ch(x)), chain)[1]    # Get the gradinte of the loss function with respect to every parameter in the chain\n",
    "\n",
    "        chain = fmap(chain, ∇) do x, dx                 # Update each parameter with a step along the gradient\n",
    "            isnothing(x)  && return dx                  # We are explicitly using functors to travers the chain and its gradient, together\n",
    "            isnothing(dx) && return x\n",
    "            x .- η .* dx\n",
    "        end\n",
    "        count = count + 1\n",
    "    end\n",
    "    \n",
    "    return chain\n",
    "end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd9967-a1b9-4eb7-9711-5ba40ea7f65c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the thing\n",
    "\n",
    "ẋ = randn(2,1)        # Random input... we want the mapping from this to ȳ\n",
    "ẏ = [3, -1, 1003]     # Give ourselves some target to hit\n",
    "\n",
    "c = Layers.Chain(\n",
    "    Layers.Dense(2, 3, (x)->1 ./ (1 + exp.(-x))),\n",
    "    Layers.Dense(3,3))\n",
    "\n",
    "ĉ = Layers.train(ẋ, ẏ, Layers.l2, c)    # Train an optimal chain\n",
    "\n",
    "ĉ(ẋ)                                    # Look at it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
